import torch
import warnings
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset

warnings.filterwarnings("ignore", category=FutureWarning)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print("Tokenizer loaded")

try:
    with open('sample_dataset.txt', 'r', encoding='utf-8') as file:
        text = file.read()
    
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    dataset = Dataset.from_dict({"text": paragraphs})
    print(f"Loaded {len(dataset)} examples from sample_dataset.txt")
except FileNotFoundError:
    print("Error: sample_dataset.txt not found. Please create the file with your training data.")
    raise

if len(dataset) > 10:  
    dataset = dataset.train_test_split(test_size=0.1)
    train_dataset = dataset['train']
    eval_dataset = dataset['test']
    print(f"Split dataset: {len(train_dataset)} train examples, {len(eval_dataset)} validation examples")
else:
    train_dataset = dataset
    eval_dataset = dataset  
    print("Dataset too small for splitting, using same data for train and eval")

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512
    )

train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])
eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])
print("Datasets tokenized")

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  
)

model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
print("Model loaded")

training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_strategy="epoch",
    save_total_limit=2,
    logging_dir='./logs',
    logging_steps=10,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer
)
print("Starting training...")

trainer.train()

model.save_pretrained('fine_tuned_gpt2')
tokenizer.save_pretrained('fine_tuned_gpt2')
print("Model and tokenizer saved to fine_tuned_gpt2")
eval_results = trainer.evaluate()
perplexity = torch.exp(torch.tensor(eval_results['eval_loss'])).item()
print(f"Perplexity: {perplexity:.2f}")

input_text = "In the bustling city"
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
attention_mask = torch.ones(input_ids.shape, device=input_ids.device)
sample_output = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=150,
    num_beams=5,
    no_repeat_ngram_size=3,
    early_stopping=True,
    pad_token_id=tokenizer.eos_token_id,
    top_p=0.9
)

generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)
print(f"\nPrompt: {input_text}")
print(f"Generated: {generated_text}")
